{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "young-potter",
   "metadata": {},
   "source": [
    "## Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Rekognition to get bbox\n",
    "import numpy as np\n",
    "import boto3\n",
    "from PIL import Image, ImageDraw, ExifTags, ImageColor, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.rekognition import determine_color, draw_animal_count\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from utils.config import *\n",
    "from utils.fix_annotation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "employed-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process whole image.py to get key points\n",
    "import mmcv\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmcv.runner import load_checkpoint\n",
    "import torch as tr\n",
    "#from torchvision import transforms\n",
    "from mmpose.apis import (inference, inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result)\n",
    "from mmpose.models import build_posenet\n",
    "from mmpose.datasets.pipelines import Compose\n",
    "\n",
    "FNT = ImageFont.truetype('/usr/share/fonts/default/Type1/n019004l.pfb', 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-marble",
   "metadata": {},
   "source": [
    "## Get Bounding Boxes from Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImage:\n",
    "    \"\"\"A simple pipeline to load image.\"\"\"\n",
    "\n",
    "    def __init__(self, color_type='color', channel_order='rgb'):\n",
    "        self.color_type = color_type\n",
    "        self.channel_order = channel_order\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call function to load images into results.\n",
    "        Args:\n",
    "            results (dict): A result dict contains the img_or_path.\n",
    "        Returns:\n",
    "            dict: ``results`` will be returned containing loaded image.\n",
    "        \"\"\"\n",
    "        if isinstance(results['img_or_path'], str):\n",
    "            results['image_file'] = results['img_or_path']\n",
    "            img = mmcv.imread(results['img_or_path'], self.color_type,\n",
    "                              self.channel_order)\n",
    "        elif isinstance(results['img_or_path'], np.ndarray):\n",
    "            results['image_file'] = ''\n",
    "            if self.color_type == 'color' and self.channel_order == 'rgb':\n",
    "                img = cv2.cvtColor(results['img_or_path'], cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            raise TypeError('\"img_or_path\" must be a numpy array or a str or '\n",
    "                            'a pathlib.Path object')\n",
    "        results['img'] = img\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "important-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pose_model(config, checkpoint=None, device='cuda:0'):\n",
    "    \"\"\"Initialize a pose model from config file.\n",
    "    Args:\n",
    "        config (str or :obj:`mmcv.Config`): Config file path or the config\n",
    "            object.\n",
    "        checkpoint (str, optional): Checkpoint path. If left as None, the model\n",
    "            will not load any weights.\n",
    "    Returns:\n",
    "        nn.Module: The constructed detector.\n",
    "    \"\"\"\n",
    "    if isinstance(config, str):\n",
    "        config = mmcv.Config.fromfile(config)\n",
    "    elif not isinstance(config, mmcv.Config):\n",
    "        raise TypeError('config must be a filename or Config object, '\n",
    "                        f'but got {type(config)}')\n",
    "    config.model.pretrained = None\n",
    "    model = build_posenet(config.model)\n",
    "    if checkpoint is not None:\n",
    "        # load model checkpoint\n",
    "        load_checkpoint(model, checkpoint, map_location=device)\n",
    "    # save the config in the model for convenience\n",
    "    model.cfg = config\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "administrative-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _box2cs(cfg, box):\n",
    "    \"\"\"This encodes bbox(x,y,w,h) into (center, scale)\n",
    "    Args:\n",
    "        x, y, w, h\n",
    "    Returns:\n",
    "        tuple: A tuple containing center and scale.\n",
    "        - np.ndarray[float32](2,): Center of the bbox (x, y).\n",
    "        - np.ndarray[float32](2,): Scale of the bbox w & h.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, w, h = box[:4]\n",
    "    input_size = cfg.data_cfg['image_size']\n",
    "    aspect_ratio = input_size[0] / input_size[1]\n",
    "    center = np.array([x + w * 0.5, y + h * 0.5], dtype=np.float32)\n",
    "\n",
    "    if w > aspect_ratio * h:\n",
    "        h = w * 1.0 / aspect_ratio\n",
    "    elif w < aspect_ratio * h:\n",
    "        w = h * aspect_ratio\n",
    "\n",
    "    # pixel std is 200.0\n",
    "    scale = np.array([w / 200.0, h / 200.0], dtype=np.float32)\n",
    "\n",
    "    scale = scale * 1.25\n",
    "\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model, dataset, person_results, img_or_path):\n",
    "    bboxes = np.array([box['bbox'] for box in person_results])\n",
    "    cfg = model.cfg\n",
    "    flip_pairs = None\n",
    "    device = next(model.parameters()).device\n",
    "    channel_order = cfg.test_pipeline[0].get('channel_order', 'rgb')\n",
    "    test_pipeline = [LoadImage(channel_order=channel_order)] + cfg.test_pipeline[1:]\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    if dataset == 'AnimalHorse10Dataset':\n",
    "        flip_pairs = []\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    batch_data = []\n",
    "    for bbox in bboxes:\n",
    "        center, scale = _box2cs(cfg, bbox)\n",
    "        # prepare data\n",
    "        data = {\n",
    "            'img_or_path':\n",
    "            img_or_path,\n",
    "            'center':\n",
    "            center,\n",
    "            'scale':\n",
    "            scale,\n",
    "            'bbox_score':\n",
    "            bbox[4] if len(bbox) == 5 else 1,\n",
    "            'bbox_id':\n",
    "            0,  # need to be assigned if batch_size > 1\n",
    "            'dataset':\n",
    "            dataset,\n",
    "            'joints_3d':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'joints_3d_visible':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'rotation':\n",
    "            0,\n",
    "            'ann_info': {\n",
    "                'image_size': np.array(cfg.data_cfg['image_size']),\n",
    "                'num_joints': cfg.data_cfg['num_joints'],\n",
    "                'flip_pairs': flip_pairs\n",
    "            }\n",
    "        }\n",
    "        data = test_pipeline(data)\n",
    "        batch_data.append(data)\n",
    "    batch_data = collate(batch_data, samples_per_gpu=1)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter not work so just move image to cuda device\n",
    "        batch_data['img'] = batch_data['img'].to(device)\n",
    "    # get all img_metas of each bounding box\n",
    "    batch_data['img_metas'] = [\n",
    "        img_metas[0] for img_metas in batch_data['img_metas'].data\n",
    "    ]\n",
    "\n",
    "    with tr.no_grad():\n",
    "        result = model(\n",
    "            img=batch_data['img'],\n",
    "            #img = torch_data,\n",
    "            img_metas=batch_data['img_metas'],\n",
    "            return_loss=False,\n",
    "            return_heatmap=False)\n",
    "    return result['preds'], result['output_heatmap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "periodic-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "device = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_head = init_pose_model(config='../my_configs/horse/resnet_50_0/head_front_grey.py', checkpoint='../train_result/horse/resnet_50_0/head_front_grey/best.pth', device = device)\n",
    "model_head_left = init_pose_model(config='../my_configs/horse/resnet_50_0/head_left_grey.py', checkpoint='../train_result/horse/resnet_50_0/head_left_grey/best.pth', device = device)\n",
    "model_head_right = init_pose_model(config='../my_configs/horse/resnet_50_0/head_right_black.py', checkpoint='../train_result/horse/resnet_50_0/head_right_black/best.pth', device = device)\n",
    "model_spine = init_pose_model(config='../my_configs/horse/resnet_50_10/spine_grey.py', checkpoint='../train_result/horse/resnet_50_10/spine_grey/best.pth', device = device)\n",
    "model_tail = init_pose_model(config='../my_configs/horse/resnet_50_5/tail_grey.py', checkpoint='../train_result/horse/resnet_50_5/tail_grey/best.pth', device = device)\n",
    "model_leg_front = init_pose_model(config='../my_configs/horse/resnet_50_5/leg_front_grey.py', checkpoint='../train_result/horse/resnet_50_5/leg_front_grey/best.pth', device = device)\n",
    "model_leg_back = init_pose_model(config='../my_configs/horse/resnet_50_5/leg_back_grey.py', checkpoint='../train_result/horse/resnet_50_5/leg_back_grey/best.pth', device = device)\n",
    "\n",
    "dataset_head = model_head.cfg.data['test']['type']\n",
    "dataset_head_left = model_head_left.cfg.data['test']['type']\n",
    "dataset_head_right = model_head_left.cfg.data['test']['type']\n",
    "dataset_spine = model_spine.cfg.data['test']['type']\n",
    "dataset_tail = model_tail.cfg.data['test']['type']\n",
    "dataset_leg_front = model_leg_front.cfg.data['test']['type']\n",
    "dataset_leg_back = model_leg_back.cfg.data['test']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brutal-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# model_head = init_pose_model(config='../my_configs/cattle/resnet_50_10/head_black.py', checkpoint='../train_result/cattle/resnet_50_10/head_black/best.pth', device = device)\n",
    "# model_spine = init_pose_model(config='../my_configs/cattle/resnet_50_5/spine_black.py', checkpoint='../train_result/cattle/resnet_50_5/spine_black/best.pth', device = device)\n",
    "# model_tail = init_pose_model(config='../my_configs/cattle/resnet_50_5/tail_black.py', checkpoint='../train_result/cattle/resnet_50_5/tail_black/best.pth', device = device)\n",
    "# model_leg_front = init_pose_model(config='../my_configs/cattle/resnet_50_10/leg_front_black.py', checkpoint='../train_result/cattle/resnet_50_10/leg_front_black/best.pth', device = device)\n",
    "# model_leg_back = init_pose_model(config='../my_configs/cattle/resnet_50_5/leg_back_black.py', checkpoint='../train_result/cattle/resnet_50_5/leg_back_black/best.pth', device = device)\n",
    "\n",
    "# dataset_head = model_head.cfg.data['test']['type']\n",
    "# dataset_spine = model_spine.cfg.data['test']['type']\n",
    "# dataset_tail = model_tail.cfg.data['test']['type']\n",
    "# dataset_leg_front = model_leg_front.cfg.data['test']['type']\n",
    "# dataset_leg_back = model_leg_back.cfg.data['test']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_color(label):\n",
    "    # BGR\n",
    "    color = (0, 0, 255)\n",
    "    if label == 'Head':\n",
    "        color = ['#EC51F8', '#74F54B', \n",
    "                 '#EC51F8', '#74F54B',\n",
    "                 '#4394F9', '#F49736',\n",
    "                 '#F49736', '#FFFB56',\n",
    "                 '#FFFB56', '#4394F9',\n",
    "                 '#07178D']\n",
    "    elif label == 'Spine':\n",
    "          color = ['#4394F9', '#4394F9', '#4394F9',\n",
    "                  '#4394F9', '#4394F9', '#4394F9', \n",
    "                  '#4394F9', '#4394F9', '#4394F9',\n",
    "                  '#4394F9', '#24518D']\n",
    "    elif label == 'Tail':\n",
    "        color = ['#EC51F8', '#EC51F8',\n",
    "                '#EC51F8', '#EC51F8',\n",
    "                '#EC51F8', '#892B8E']\n",
    "    elif label == 'Leg_front':\n",
    "        color = ['#F49736', '#F49736',\n",
    "                '#F49736', '#F49736',\n",
    "                '#F49736', '#F49736',\n",
    "                '#F49736', '#F49736',\n",
    "                '#F49736', '#8C551E']\n",
    "    elif label == 'Leg_back':\n",
    "        color = ['#74F54B', '#74F54B',\n",
    "                '#74F54B', '#74F54B',\n",
    "                '#74F54B', '#74F54B',\n",
    "                '#74F54B', '#74F54B',\n",
    "                '#74F54B', '#3F8D28',]\n",
    "    elif label == 'Head Left' or label == 'Head Right':\n",
    "        color = ['#EC51F8', '#74F54B',\n",
    "                 '#F49736', '#FFFB56',\n",
    "                 '#4394F9', '#4394F9',\n",
    "                 '#07178D']\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grand-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skeleton(label):\n",
    "    skeleton_list = []\n",
    "    if label == 'Head':\n",
    "        skeleton_list = [[4, 0], [4, 2], [0, 2], [1, 3], \n",
    "                        [5, 6], [7, 8], [0, 1], [1, 5],\n",
    "                        [5, 7], [7, 9], [2, 3], [3, 6],\n",
    "                        [6, 8], [8, 9], [4, 9]]\n",
    "    elif label == 'Spine':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]]\n",
    "    elif label == 'Tail':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4]]\n",
    "    elif label == 'Leg_front':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7], [7, 8]]\n",
    "    elif label == 'Leg_back':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7], [7, 8]]\n",
    "    elif label == 'Head Left' or label == 'Head Right':\n",
    "        skeleton_list = [[0, 1], [1, 2], [1, 5], [2, 3], \n",
    "                         [2, 4], [2, 5], [3, 4], [4, 5]]\n",
    "    return skeleton_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "underlying-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_bgr(color):\n",
    "    color = list(color)\n",
    "    temp_r = color[0]\n",
    "    color[0] = color[2]\n",
    "    color[2] = temp_r\n",
    "    return tuple(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minimal-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_pose(points, draw, label, temp_bbox, size, scale_rate):\n",
    "    #print(temp_bbox, size, scale_rate, label)\n",
    "    points = points[0]\n",
    "#     if label == 'Leg_front':\n",
    "#         print('Leg_front')\n",
    "#         print(points)\n",
    "#     if label == 'Tail' or label == 'Leg_front' or label == 'Leg_back':\n",
    "#         print(label)\n",
    "#         print(points)\n",
    "#     if label == 'Leg_front' or label == 'Leg_back':\n",
    "#         return draw\n",
    "    CS_THR = 0.4\n",
    "    # keypoints\n",
    "    kp_color = get_kp_color(label)\n",
    "    # connect line\n",
    "    skeleton_list = get_skeleton(label)\n",
    "    for ske in skeleton_list:\n",
    "        fir_pt_x, fir_pt_y, fir_pt_p = points[ske[0]]\n",
    "        sec_pt_x, sec_pt_y, sec_pt_p = points[ske[1]]\n",
    "        if fir_pt_p > CS_THR and sec_pt_p > CS_THR:\n",
    "#             shape = [(fir_pt_x, fir_pt_y), (sec_pt_x, sec_pt_y)]\n",
    "#             print('before shape')\n",
    "#             print(shape)\n",
    "            if fir_pt_x < size[0] and fir_pt_y < size[1] and sec_pt_x < size[0] and sec_pt_y < size[1]:\n",
    "#                 shape = [(fir_pt_x, fir_pt_y), (sec_pt_x, sec_pt_y)]\n",
    "#                 print('shape')\n",
    "#                 print(shape)\n",
    "                fir_pt_x = temp_bbox[0] + fir_pt_x / scale_rate[0]\n",
    "                fir_pt_y = temp_bbox[1] + fir_pt_y / scale_rate[1]\n",
    "                sec_pt_x = temp_bbox[0] + sec_pt_x / scale_rate[0]\n",
    "                sec_pt_y = temp_bbox[1] + sec_pt_y / scale_rate[1]\n",
    "                shape = [(fir_pt_x, fir_pt_y), (sec_pt_x, sec_pt_y)]\n",
    "#                 print('scaled shape')\n",
    "#                 print(shape)\n",
    "                draw.line(shape, fill=kp_color[-1], width=8)\n",
    "    for i, point in enumerate(points):\n",
    "        x, y, p = point\n",
    "        if p > CS_THR and x < size[0] and y < size[1]:\n",
    "            x = temp_bbox[0] + x / scale_rate[0]\n",
    "            y = temp_bbox[1] + y / scale_rate[1]\n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            draw.ellipse([(x-11, y-11), (x+11, y+11)], fill=kp_color[-1], outline=None)\n",
    "            draw.ellipse([(x-6, y-6), (x+6, y+6)], fill=kp_color[i], outline=None)\n",
    "            #draw.text((x-40, y-40), '{}%'.format(int(p*100)), font=FNT, fill=(255, 255, 255))\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "thick-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_bbox(left, top, width, height, extend_rate):\n",
    "    temp_left = left - (width * extend_rate / 2)\n",
    "    temp_top = top - (height * extend_rate / 2)\n",
    "    temp_width = width + (width * extend_rate / 2)\n",
    "    temp_height = height + (height * extend_rate / 2)\n",
    "    return temp_left, temp_top, temp_width, temp_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "still-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(name):\n",
    "    \n",
    "    if name == 'horse':\n",
    "        color = '#FF9300'\n",
    "    elif name == 'head' or name == \"head left\" or name == \"head right\":\n",
    "        color = '#0096FF'\n",
    "    elif name == 'tag':\n",
    "        color = '#00FFFF'\n",
    "    elif name == 'knee':\n",
    "        color = '#FFFB00'\n",
    "    elif name == 'hoof':\n",
    "        color = '#00F900'\n",
    "    elif name == 'tail':\n",
    "        color = '#FF40FF'\n",
    "    elif name == 'side left' or name == 'side right':\n",
    "        color = '#FF2600'\n",
    "    elif name == 'udder':\n",
    "        color = '#9437FF'\n",
    "    elif name == 'teat':\n",
    "        color = '#FF2F92'\n",
    "    else:\n",
    "        color = '#000000'\n",
    "\n",
    "    return color\n",
    "\n",
    "def get_opacity(name):\n",
    "    \n",
    "    if name == 'horse':\n",
    "        opacity = 0.3\n",
    "    elif name == 'tag':\n",
    "        opacity = 0.3\n",
    "    elif name == 'head' or name == \"head left\" or name == \"head right\":\n",
    "        opacity = 0.45\n",
    "    elif name == 'knee':\n",
    "        opacity = 0.3\n",
    "    elif name == 'hoof':\n",
    "        opacity = 0.35\n",
    "    elif name == 'tail':\n",
    "        opacity = 0.3\n",
    "    elif name == 'side left' or name == 'side right':\n",
    "        opacity = 0.3\n",
    "    elif name == 'udder':\n",
    "        opacity = 0.35\n",
    "    elif name == 'teat':\n",
    "        opacity = 0.3\n",
    "    else:\n",
    "        opacity = 0.0\n",
    "\n",
    "    return opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "celtic-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_cut_off(name):\n",
    "    if name == 'horse':\n",
    "        confidence = 50\n",
    "    elif name == 'tag':\n",
    "        confidence = 86.9\n",
    "    elif name == 'head':\n",
    "        confidence = 80\n",
    "    elif name == 'knee':\n",
    "        confidence = 70.0\n",
    "    elif name == 'hoof':\n",
    "        confidence = 92.9\n",
    "    elif name == 'tail':\n",
    "        confidence = 80\n",
    "    elif name == 'udder':\n",
    "        confidence = 35.0\n",
    "    elif name == 'teat':\n",
    "        confidence = 73.0\n",
    "    else:\n",
    "        confidence = 80.0\n",
    "\n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interior-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bbox, color):\n",
    "    temp_bbox = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
    "    ori_image = Image.fromarray(image, 'RGB')    \n",
    "    cropped_image = ori_image.copy().crop(temp_bbox)\n",
    "    # rescale images and annotations\n",
    "    size_x = 256\n",
    "    size_y = 256\n",
    "    \n",
    "    # Image.ANTIALIAS scale the cropped image (head)\n",
    "    ori_crop_size = cropped_image.size\n",
    "    cropped_image.thumbnail((size_x, size_y), Image.ANTIALIAS)\n",
    "    # scale keypoints\n",
    "    new_image = Image.new('RGB', (size_x, size_y), color = color)\n",
    "    new_image.paste(cropped_image)\n",
    "    cropped_image_size = cropped_image.size\n",
    "    scale_rate = (cropped_image_size[0]/ori_crop_size[0], cropped_image_size[1]/ori_crop_size[1])\n",
    "    new_image = np.array(new_image)\n",
    "    return new_image, temp_bbox, cropped_image_size, scale_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "trained-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_count = 0\n",
    "#draw response\n",
    "def draw_response(image, response, animal_target, draw_boundary=True, fill=True, draw_btn=True):\n",
    "    global tail_count\n",
    "    tail_check = False\n",
    "    temp_image = image.copy()\n",
    "    b, g, r = image.split()\n",
    "    image = Image.merge(\"RGB\", (r, g, b))\n",
    "    # original image size\n",
    "    draw = ImageDraw.Draw(image, mode='RGBA')\n",
    "    # bbox\n",
    "    for customLabel in response['CustomLabels']:\n",
    "        if 'Geometry' in customLabel:\n",
    "            box = customLabel['Geometry']['BoundingBox']\n",
    "            left, top, width, height = extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], 0)\n",
    "            label = customLabel['Name'].lower()\n",
    "            if label == 'Udder':\n",
    "                print('Udder')\n",
    "            elif label == 'Teat':\n",
    "                print('Teat')\n",
    "            conf_cut = get_confidence_cut_off(label)\n",
    "            # skip current label\n",
    "            if customLabel['Confidence'] < conf_cut:\n",
    "                continue\n",
    "            #draw bbox\n",
    "            color = get_color(label)\n",
    "            opacity = round(get_opacity(label) * 255)\n",
    "            if draw_boundary and fill:  \n",
    "                draw.rectangle(xy=[(left, top), (left+width, top+height)], outline=color, fill=color+f'{opacity:0>2X}', width=3)\n",
    "            elif fill:\n",
    "                draw.rectangle(xy=[(left, top), (left+width, top+height)], outline=None, fill=color+f'{opacity:0>2X}', width=3)\n",
    "            elif draw_boundary:\n",
    "                draw.rectangle(xy=[(left, top), (left+width, top+height)], outline=color, fill=None, width=3)\n",
    "            if draw_btn:\n",
    "                text_width, text_height = FNT.getsize(label)\n",
    "                draw.rectangle(xy=[(left, top), (left+text_width, top+text_height)], outline=None, fill=color, width=3)\n",
    "                draw.text((left, top), label, fill='#000000', font=FNT)\n",
    "    #keypoints\n",
    "    for customLabel in response['CustomLabels']:\n",
    "        if 'Geometry' in customLabel:\n",
    "            box = customLabel['Geometry']['BoundingBox']\n",
    "            left, top, width, height = extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], 0)\n",
    "            label = customLabel['Name'].lower()\n",
    "            #lower case for comparison\n",
    "            conf_cut = get_confidence_cut_off(label)\n",
    "            # skip current label\n",
    "            if customLabel['Confidence'] < conf_cut:\n",
    "                continue\n",
    "        #***** Keypoints\n",
    "            if label == 'head':\n",
    "                extend_rate = 0.00\n",
    "                np_image = np.array(temp_image)                \n",
    "                head_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cropped_image, temp_bbox, size, boundary = crop_image(np_image, head_bbox, color='grey')\n",
    "                head_result = []\n",
    "                head_result.append({'bbox': [0, 0, 255, 255]})\n",
    "                preds, _ = process_model(model_head, dataset_head, head_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Head', temp_bbox, size, boundary)\n",
    "            elif label == 'head left':\n",
    "                extend_rate = 0.00\n",
    "                np_image = np.array(temp_image)                \n",
    "                head_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cropped_image, temp_bbox, size, boundary = crop_image(np_image, head_bbox, color='grey')\n",
    "                head_result = []\n",
    "                head_result.append({'bbox': [0, 0, 255, 255]})\n",
    "                preds, _ = process_model(model_head_left, dataset_head_left, head_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Head Left', temp_bbox, size, boundary)\n",
    "            elif label == 'head right':\n",
    "                extend_rate = 0.00\n",
    "                np_image = np.array(temp_image)                \n",
    "                head_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cropped_image, temp_bbox, size, boundary = crop_image(np_image, head_bbox, color='black')\n",
    "                head_result = []\n",
    "                head_result.append({'bbox': [0, 0, 255, 255]})\n",
    "                preds, _ = process_model(model_head_right, dataset_head_right, head_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Head Right', temp_bbox, size, boundary)\n",
    "            elif label == 'horse':\n",
    "                extend_rate = 0.05\n",
    "                np_image = np.array(temp_image)\n",
    "                horse_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cropped_image, temp_bbox, size, boundary = crop_image(np_image, horse_bbox, color='grey')\n",
    "                cow_result = []\n",
    "                cow_result.append({'bbox': [0, 0, 255, 255]})\n",
    "                # spine\n",
    "                preds, _ = process_model(model_spine, dataset_spine, cow_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Spine', temp_bbox, size, boundary)\n",
    "                # leg front\n",
    "                preds, _ = process_model(model_leg_front, dataset_leg_front, cow_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Leg_front', temp_bbox, size, boundary)\n",
    "                # leg back\n",
    "                preds, _ = process_model(model_leg_back, dataset_leg_back, cow_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Leg_back', temp_bbox, size, boundary)\n",
    "            elif label == 'tail':\n",
    "                extend_rate = 0.05\n",
    "                np_image = np.array(temp_image)\n",
    "                tail_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cropped_image, temp_bbox, size, boundary = crop_image(np_image, tail_bbox, color='grey')\n",
    "                tail_result = []\n",
    "                tail_result.append({'bbox': [0, 0, 255, 255]})\n",
    "                preds, _ = process_model(model_tail, dataset_tail, tail_result, cropped_image)\n",
    "                draw = vis_pose(preds, draw, 'Tail', temp_bbox, size, boundary)\n",
    "                tail_check = True\n",
    "#*****\n",
    "    \n",
    "    \n",
    "    img = np.asarray(image)[:,:,::-1].copy()\n",
    "    inferred_frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return inferred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "embedded-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeVideo(src_video, src_bbox_json, src_img_dir, output_file, fps=5):\n",
    "    \n",
    "    start = time.time()\n",
    "        #imgWidth, imgHeight = image.size\n",
    "    with Image.open(src_img_dir+'0.jpg') as img:\n",
    "        imgWidth, imgHeight = img.size\n",
    "        imgSize = (imgWidth, imgHeight)\n",
    "        img.close()\n",
    "    cap = cv2.VideoCapture(src_video)\n",
    "    frameRate = cap.get(fps) #frame rate\n",
    "    print('FrameRate:', frameRate)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    videoWriter = cv2.VideoWriter(output_file, fourcc, frameRate, imgSize) \n",
    "    \n",
    "    with open(src_bbox_json) as bbox_json:\n",
    "        bbox_frames = json.load(bbox_json)\n",
    "        for frameId, bbox_data in enumerate(bbox_frames['Frames']):\n",
    "            # get each image frame\n",
    "            with Image.open(src_img_dir+str(frameId)+'.jpg') as img:\n",
    "                inferred_frame = draw_response(img, bbox_data, animal_target='cow')\n",
    "                inferred_frame = cv2.cvtColor(inferred_frame, cv2.COLOR_BGR2RGB)\n",
    "#                 if frameId == 100:\n",
    "#                     break\n",
    "                # check each 50 frame\n",
    "                if frameId % 50 == 0:\n",
    "                    print(\"Finish Processing {} frame\".format(frameId))\n",
    "                    plt.imshow(inferred_frame)\n",
    "                    plt.title(\"Frame {}\".format(int(frameId)))\n",
    "                    plt.savefig('debug_imgs/check_{}.jpg'.format(frameId), dpi=200)\n",
    "                    lap = time.time()\n",
    "                    print('lap time: ', lap - start)\n",
    "                videoWriter.write(inferred_frame)\n",
    "                img.close()\n",
    "\n",
    "    videoWriter.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    bbox_json.close()\n",
    "    \n",
    "    #end time\n",
    "    end = time.time()\n",
    "    print('total time lapse', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "strange-algebra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_data/inferred_video/inferred_Sigar.mp4\n",
      "FrameRate: 29.976649153531817\n",
      "Finish Processing 0 frame\n",
      "lap time:  1.775212287902832\n",
      "Finish Processing 50 frame\n",
      "lap time:  12.535423040390015\n",
      "Finish Processing 100 frame\n",
      "lap time:  24.892287492752075\n",
      "Finish Processing 150 frame\n",
      "lap time:  36.96570825576782\n",
      "Finish Processing 200 frame\n",
      "lap time:  49.807860374450684\n",
      "Finish Processing 250 frame\n",
      "lap time:  62.8437933921814\n",
      "Finish Processing 300 frame\n",
      "lap time:  75.91395401954651\n",
      "Finish Processing 350 frame\n",
      "lap time:  89.50206875801086\n",
      "Finish Processing 400 frame\n",
      "lap time:  103.379159450531\n",
      "Finish Processing 450 frame\n",
      "lap time:  117.45967626571655\n",
      "Finish Processing 500 frame\n",
      "lap time:  131.6019425392151\n",
      "Finish Processing 550 frame\n",
      "lap time:  144.7317454814911\n",
      "Finish Processing 600 frame\n",
      "lap time:  157.3463110923767\n",
      "Finish Processing 650 frame\n",
      "lap time:  170.71027731895447\n",
      "Finish Processing 700 frame\n",
      "lap time:  186.7791519165039\n",
      "Finish Processing 750 frame\n",
      "lap time:  203.54722261428833\n",
      "Finish Processing 800 frame\n",
      "lap time:  224.5780110359192\n",
      "Finish Processing 850 frame\n",
      "lap time:  245.77411603927612\n",
      "Finish Processing 900 frame\n",
      "lap time:  258.5495035648346\n",
      "Finish Processing 950 frame\n",
      "lap time:  271.8022286891937\n",
      "Finish Processing 1000 frame\n",
      "lap time:  287.72616267204285\n",
      "Finish Processing 1050 frame\n",
      "lap time:  305.1870355606079\n",
      "Finish Processing 1100 frame\n",
      "lap time:  322.2297430038452\n",
      "Finish Processing 1150 frame\n",
      "lap time:  336.2851526737213\n",
      "Finish Processing 1200 frame\n",
      "lap time:  352.90865540504456\n",
      "Finish Processing 1250 frame\n",
      "lap time:  373.4966015815735\n",
      "Finish Processing 1300 frame\n",
      "lap time:  389.1514415740967\n",
      "Finish Processing 1350 frame\n",
      "lap time:  405.46040964126587\n",
      "Finish Processing 1400 frame\n",
      "lap time:  427.703679561615\n",
      "Finish Processing 1450 frame\n",
      "lap time:  449.18066120147705\n",
      "Finish Processing 1500 frame\n",
      "lap time:  470.78350162506104\n",
      "Finish Processing 1550 frame\n",
      "lap time:  492.5124022960663\n",
      "Finish Processing 1600 frame\n",
      "lap time:  513.5161037445068\n",
      "Finish Processing 1650 frame\n",
      "lap time:  530.4793927669525\n",
      "Finish Processing 1700 frame\n",
      "lap time:  545.240888595581\n",
      "Finish Processing 1750 frame\n",
      "lap time:  557.2266156673431\n",
      "Finish Processing 1800 frame\n",
      "lap time:  568.8811943531036\n",
      "Finish Processing 1850 frame\n",
      "lap time:  581.3032424449921\n",
      "Finish Processing 1900 frame\n",
      "lap time:  596.8823788166046\n",
      "Finish Processing 1950 frame\n",
      "lap time:  616.9486746788025\n",
      "Finish Processing 2000 frame\n",
      "lap time:  637.0674362182617\n",
      "Finish Processing 2050 frame\n",
      "lap time:  651.907288312912\n",
      "total time lapse 652.1919660568237\n",
      "finished analyzing the video Sigar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_name_list = ['Sigar']\n",
    "video_format = ['.mov']\n",
    "for v_idx, video in enumerate(video_name_list):\n",
    "    src_video = 'video_data/input_video/'+video+video_format[v_idx]\n",
    "    src_bbox_json = 'json_data/'+video+'_horse_bbox.json'\n",
    "    src_img_dir = 'frame_img/'+video+'/'\n",
    "    output_video = 'video_data/inferred_video/inferred_'+video+'.mp4'\n",
    "    print(output_video)\n",
    "    analyzeVideo(src_video, src_bbox_json, src_img_dir, output_video)\n",
    "    print('finished analyzing the video '+video)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-review",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36] *",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
