{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "young-potter",
   "metadata": {},
   "source": [
    "## Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Rekognition to get bbox\n",
    "import numpy as np\n",
    "import boto3\n",
    "from PIL import Image, ImageDraw, ExifTags, ImageColor, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.rekognition import determine_color, draw_animal_count\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from utils.config import *\n",
    "from utils.fix_annotation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "employed-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process whole image.py to get key points\n",
    "import mmcv\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmcv.runner import load_checkpoint\n",
    "import torch as tr\n",
    "#from torchvision import transforms\n",
    "from mmpose.apis import (inference, inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result)\n",
    "from mmpose.models import build_posenet\n",
    "from mmpose.datasets.pipelines import Compose\n",
    "\n",
    "FNT = ImageFont.truetype('/usr/share/fonts/default/Type1/n019004l.pfb', 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-marble",
   "metadata": {},
   "source": [
    "## Get Bounding Boxes from Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImage:\n",
    "    \"\"\"A simple pipeline to load image.\"\"\"\n",
    "\n",
    "    def __init__(self, color_type='color', channel_order='rgb'):\n",
    "        self.color_type = color_type\n",
    "        self.channel_order = channel_order\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call function to load images into results.\n",
    "        Args:\n",
    "            results (dict): A result dict contains the img_or_path.\n",
    "        Returns:\n",
    "            dict: ``results`` will be returned containing loaded image.\n",
    "        \"\"\"\n",
    "        if isinstance(results['img_or_path'], str):\n",
    "            results['image_file'] = results['img_or_path']\n",
    "            img = mmcv.imread(results['img_or_path'], self.color_type,\n",
    "                              self.channel_order)\n",
    "        elif isinstance(results['img_or_path'], np.ndarray):\n",
    "            results['image_file'] = ''\n",
    "            if self.color_type == 'color' and self.channel_order == 'rgb':\n",
    "                img = cv2.cvtColor(results['img_or_path'], cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            raise TypeError('\"img_or_path\" must be a numpy array or a str or '\n",
    "                            'a pathlib.Path object')\n",
    "        results['img'] = img\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "important-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pose_model(config, checkpoint=None, device='cuda:0'):\n",
    "    \"\"\"Initialize a pose model from config file.\n",
    "    Args:\n",
    "        config (str or :obj:`mmcv.Config`): Config file path or the config\n",
    "            object.\n",
    "        checkpoint (str, optional): Checkpoint path. If left as None, the model\n",
    "            will not load any weights.\n",
    "    Returns:\n",
    "        nn.Module: The constructed detector.\n",
    "    \"\"\"\n",
    "    if isinstance(config, str):\n",
    "        config = mmcv.Config.fromfile(config)\n",
    "    elif not isinstance(config, mmcv.Config):\n",
    "        raise TypeError('config must be a filename or Config object, '\n",
    "                        f'but got {type(config)}')\n",
    "    config.model.pretrained = None\n",
    "    model = build_posenet(config.model)\n",
    "    if checkpoint is not None:\n",
    "        # load model checkpoint\n",
    "        load_checkpoint(model, checkpoint, map_location=device)\n",
    "    # save the config in the model for convenience\n",
    "    model.cfg = config\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "administrative-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _box2cs(cfg, box):\n",
    "    \"\"\"This encodes bbox(x,y,w,h) into (center, scale)\n",
    "    Args:\n",
    "        x, y, w, h\n",
    "    Returns:\n",
    "        tuple: A tuple containing center and scale.\n",
    "        - np.ndarray[float32](2,): Center of the bbox (x, y).\n",
    "        - np.ndarray[float32](2,): Scale of the bbox w & h.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, w, h = box[:4]\n",
    "    input_size = cfg.data_cfg['image_size']\n",
    "    aspect_ratio = input_size[0] / input_size[1]\n",
    "    center = np.array([x + w * 0.5, y + h * 0.5], dtype=np.float32)\n",
    "\n",
    "    if w > aspect_ratio * h:\n",
    "        h = w * 1.0 / aspect_ratio\n",
    "    elif w < aspect_ratio * h:\n",
    "        w = h * aspect_ratio\n",
    "\n",
    "    # pixel std is 200.0\n",
    "    scale = np.array([w / 200.0, h / 200.0], dtype=np.float32)\n",
    "\n",
    "    scale = scale * 1.25\n",
    "\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model, dataset, person_results, img_or_path):\n",
    "    bboxes = np.array([box['bbox'] for box in person_results])\n",
    "    cfg = model.cfg\n",
    "    flip_pairs = None\n",
    "    device = next(model.parameters()).device\n",
    "    channel_order = cfg.test_pipeline[0].get('channel_order', 'rgb')\n",
    "    test_pipeline = [LoadImage(channel_order=channel_order)] + cfg.test_pipeline[1:]\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    if dataset == 'AnimalHorse10Dataset':\n",
    "        flip_pairs = []\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    batch_data = []\n",
    "    for bbox in bboxes:\n",
    "        center, scale = _box2cs(cfg, bbox)\n",
    "        # prepare data\n",
    "        data = {\n",
    "            'img_or_path':\n",
    "            img_or_path,\n",
    "            'center':\n",
    "            center,\n",
    "            'scale':\n",
    "            scale,\n",
    "            'bbox_score':\n",
    "            bbox[4] if len(bbox) == 5 else 1,\n",
    "            'bbox_id':\n",
    "            0,  # need to be assigned if batch_size > 1\n",
    "            'dataset':\n",
    "            dataset,\n",
    "            'joints_3d':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'joints_3d_visible':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'rotation':\n",
    "            0,\n",
    "            'ann_info': {\n",
    "                'image_size': np.array(cfg.data_cfg['image_size']),\n",
    "                'num_joints': cfg.data_cfg['num_joints'],\n",
    "                'flip_pairs': flip_pairs\n",
    "            }\n",
    "        }\n",
    "        data = test_pipeline(data)\n",
    "        batch_data.append(data)\n",
    "    batch_data = collate(batch_data, samples_per_gpu=1)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter not work so just move image to cuda device\n",
    "        batch_data['img'] = batch_data['img'].to(device)\n",
    "    # get all img_metas of each bounding box\n",
    "    batch_data['img_metas'] = [\n",
    "        img_metas[0] for img_metas in batch_data['img_metas'].data\n",
    "    ]\n",
    "\n",
    "    with tr.no_grad():\n",
    "        result = model(\n",
    "            img=batch_data['img'],\n",
    "            #img = torch_data,\n",
    "            img_metas=batch_data['img_metas'],\n",
    "            return_loss=False,\n",
    "            return_heatmap=False)\n",
    "    return result['preds'], result['output_heatmap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "periodic-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n",
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "device = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_head = init_pose_model(config='../myConfigs/train_head_resnet.py', checkpoint='../temp_logs/cattle_head/resnet/best.pth', device = device)\n",
    "model_spine = init_pose_model(config='../myConfigs/train_spine_resnet.py', checkpoint='../temp_logs/cattle_spine/resnet/best.pth', device = device)\n",
    "model_tail = init_pose_model(config='../myConfigs/train_tail_ori_resnet.py', checkpoint='../temp_logs/cattle_tail_ori/resnet/best.pth', device = device)\n",
    "model_leg_front = init_pose_model(config='../myConfigs/train_leg_front_resnet.py', checkpoint='../temp_logs/cattle_leg_front/resnet/best.pth', device = device)\n",
    "model_leg_back = init_pose_model(config='../myConfigs/train_leg_back_resnet.py', checkpoint='../temp_logs/cattle_leg_back/resnet/best.pth', device = device)\n",
    "\n",
    "dataset_head = model_head.cfg.data['test']['type']\n",
    "dataset_spine = model_spine.cfg.data['test']['type']\n",
    "dataset_tail = model_tail.cfg.data['test']['type']\n",
    "dataset_leg_front = model_leg_front.cfg.data['test']['type']\n",
    "dataset_leg_back = model_leg_back.cfg.data['test']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "printable-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_color(label):\n",
    "    # BGR\n",
    "    color = (0, 0, 255)\n",
    "    if label == 'Head':\n",
    "        color = [(236, 81, 248), (116, 245, 75), \n",
    "                 (236, 81, 248), (116, 245, 75),\n",
    "                 (67, 148, 249), (244, 151, 54),\n",
    "                 (244, 151, 54), (255, 251, 86),\n",
    "                 (255, 251, 86), (67, 148, 249),\n",
    "                 (7, 23, 141)]\n",
    "    elif label == 'Spine':\n",
    "        color = [(67, 148, 249), (67, 148, 249),\n",
    "                (67, 148, 249), (67, 148, 249),\n",
    "                (67, 148, 249), (67, 148, 249),\n",
    "                (67, 148, 249), (67, 148, 249),\n",
    "                (36, 81, 141)]\n",
    "    elif label == 'Tail':\n",
    "        color = [(236, 81, 248), (236, 81, 248),\n",
    "                (236, 81, 248), (236, 81, 248),\n",
    "                (236, 81, 248), (134, 43, 142)]\n",
    "    elif label == 'Leg_front':\n",
    "        color = [(244, 151, 54), (244, 151, 54),\n",
    "                (244, 151, 54), (244, 151, 54),\n",
    "                (244, 151, 54), (244, 151, 54),\n",
    "                (244, 151, 54), (244, 151, 54),\n",
    "                (244, 151, 54), (140, 80, 30)]\n",
    "    elif label == 'Leg_back':\n",
    "        color = [(116, 245, 75), (116, 245, 75),\n",
    "                (116, 245, 75), (116, 245, 75),\n",
    "                (116, 245, 75), (116, 245, 75),\n",
    "                (116, 245, 75), (116, 245, 75),\n",
    "                (116, 245, 75), (63, 141, 40)]\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grand-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skeleton(label):\n",
    "    skeleton_list = []\n",
    "    if label == 'Head':\n",
    "        skeleton_list = [[4, 0], [4, 2], [0, 2], [1, 3], \n",
    "                        [5, 6], [7, 8], [0, 1], [1, 5],\n",
    "                        [5, 7], [7, 9], [2, 3], [3, 6],\n",
    "                        [6, 8], [8, 9], [4, 9]]\n",
    "    elif label == 'Spine':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7]]\n",
    "    elif label == 'Tail':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4]]\n",
    "    elif label == 'Leg_front':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7], [7, 8]]\n",
    "    elif label == 'Leg_back':\n",
    "        skeleton_list = [[0, 1], [1, 2], [2, 3], [3, 4],\n",
    "                        [4, 5], [5, 6], [6, 7], [7, 8]]\n",
    "    return skeleton_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "underlying-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_bgr(color):\n",
    "    color = list(color)\n",
    "    temp_r = color[0]\n",
    "    color[0] = color[2]\n",
    "    color[2] = temp_r\n",
    "    return tuple(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "minimal-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_pose(img, points, draw, label):\n",
    "    points = points[0]\n",
    "#     if label == 'Tail' or label == 'Leg_front' or label == 'Leg_back':\n",
    "#         print(label)\n",
    "#         print(points)\n",
    "#     if label == 'Leg_front' or label == 'Leg_back':\n",
    "#         return draw\n",
    "    CS_THR = 0.4\n",
    "    # keypoints\n",
    "    kp_color = get_kp_color(label)\n",
    "    # connect line\n",
    "    skeleton_list = get_skeleton(label)\n",
    "    for ske in skeleton_list:\n",
    "        #print(points)\n",
    "        fir_pt_x, fir_pt_y, fir_pt_p = points[ske[0]]\n",
    "        sec_pt_x, sec_pt_y, sec_pt_p = points[ske[1]]\n",
    "        if fir_pt_p > CS_THR and sec_pt_p > CS_THR:\n",
    "            shape = [(fir_pt_x, fir_pt_y), (sec_pt_x, sec_pt_y)]\n",
    "            draw.line(shape, fill=rgb_to_bgr(kp_color[-1]), width=10)\n",
    "    for i, point in enumerate(points):\n",
    "        x, y, p = point\n",
    "        if p > CS_THR:\n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            draw.ellipse([(x-13, y-13), (x+13, y+13)], fill=rgb_to_bgr(kp_color[-1]), outline=None)\n",
    "            draw.ellipse([(x-8, y-8), (x+8, y+8)], fill=rgb_to_bgr(kp_color[i]), outline=None)\n",
    "            draw.text((x-40, y-40), '{}%'.format(int(p*100)), font=FNT, fill=(255, 255, 255))\n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_bbox(left, top, width, height, extend_rate):\n",
    "    temp_left = left - left * extend_rate\n",
    "    temp_top = top - top * extend_rate\n",
    "    temp_width = width * extend_rate + width\n",
    "    temp_height = height * extend_rate + height\n",
    "    return temp_left, temp_top, temp_width, temp_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "trained-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_count = 0\n",
    "#draw response\n",
    "def draw_response(image, response, animal_target):\n",
    "    global tail_count\n",
    "    tail_check = False\n",
    "\n",
    "    # original image size\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for customLabel in response['CustomLabels']:\n",
    "\n",
    "        if 'Geometry' in customLabel:\n",
    "            box = customLabel['Geometry']['BoundingBox']\n",
    "            left, top, width, height = extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], 0)\n",
    "            label = customLabel['Name']\n",
    "        \n",
    "            text = label\n",
    "            text_width, text_height = FNT.getsize(label)\n",
    "            color = determine_color(label, True)\n",
    "\n",
    "            button_width = int(text_width + 20)\n",
    "            button_height = int(text_height + 15)\n",
    "            button_size = (button_width, button_height)\n",
    "            button_img = Image.new('RGB', button_size, color)\n",
    "            button_draw = ImageDraw.Draw(button_img)\n",
    "            button_draw.text((10, 10), text, fill ='#000000', font=FNT)\n",
    "            image.paste(button_img, (int(left), int(top)))  \n",
    "            \n",
    "#***** Keypoints\n",
    "            if label == 'Head':\n",
    "                extend_rate = 0.05\n",
    "                np_image = np.array(image)                \n",
    "                head_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                head_result = []\n",
    "                head_result.append({'bbox': head_bbox})\n",
    "                preds, _ = process_model(model_head, dataset_head, head_result, np_image)\n",
    "                draw = vis_pose(np_image, preds, draw, 'Head')\n",
    "            elif label == 'Cow':\n",
    "                extend_rate = 0.05\n",
    "                np_image = np.array(image)\n",
    "                cow_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                cow_result = []\n",
    "                cow_result.append({'bbox': cow_bbox})\n",
    "                # spine\n",
    "                preds, _ = process_model(model_spine, dataset_spine, cow_result, np_image)\n",
    "                draw = vis_pose(np_image, preds, draw, 'Spine')\n",
    "                # leg front\n",
    "                preds, _ = process_model(model_leg_front, dataset_leg_front, cow_result, np_image)\n",
    "                draw = vis_pose(np_image, preds, draw, 'Leg_front')\n",
    "                # leg back\n",
    "                preds, _ = process_model(model_leg_back, dataset_leg_back, cow_result, np_image)\n",
    "                draw = vis_pose(np_image, preds, draw, 'Leg_back')\n",
    "            elif label == 'Tail':\n",
    "                extend_rate = 0.15\n",
    "                np_image = np.array(image)\n",
    "                tail_bbox = list(extend_bbox(box['Left'], box['Top'], box['Width'], box['Height'], extend_rate))\n",
    "                tail_result = []\n",
    "                tail_result.append({'bbox': tail_bbox})\n",
    "                preds, _ = process_model(model_tail, dataset_tail, tail_result, np_image)\n",
    "                draw = vis_pose(np_image, preds, draw, 'Tail')\n",
    "                tail_check = True\n",
    "#*****\n",
    "            points = (\n",
    "                (left, top),\n",
    "                (left + width, top),\n",
    "                (left + width, top + height),\n",
    "                (left , top + height),\n",
    "                (left, top))\n",
    "            \n",
    "            thickness = 5\n",
    "            \n",
    "            if label == 'cow':\n",
    "                thickness = 7\n",
    "                \n",
    "            draw.line(points, fill=color, width=thickness)\n",
    "    \n",
    "    img = np.asarray(image)[:,:,::-1].copy()\n",
    "    # check tail label\n",
    "    inferred_frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     if tail_check:\n",
    "#         print('Tail')\n",
    "#         plt.imshow(inferred_frame)\n",
    "#         plt.title(\"Tail Image {}\".format(tail_count))\n",
    "#         #plt.show()\n",
    "#         plt.savefig('frame_imgs/tail_{}.png'.format(tail_count), dpi=100)\n",
    "#         tail_count+=1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "embedded-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeVideo(src_video, src_bbox_json, src_img_dir, output_file, projectVersionArn, fps=5):\n",
    "    \n",
    "    start = time.time()\n",
    "        #imgWidth, imgHeight = image.size\n",
    "    with Image.open(src_img_dir+'0.jpg') as img:\n",
    "        imgWidth, imgHeight = img.size\n",
    "        imgSize = (imgWidth, imgHeight)\n",
    "        img.close()\n",
    "    cap = cv2.VideoCapture(src_video)\n",
    "    frameRate = cap.get(fps) #frame rate\n",
    "    print('FrameRate:', frameRate)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    videoWriter = cv2.VideoWriter(output_file, fourcc, frameRate, imgSize) \n",
    "    \n",
    "    with open(src_bbox_json) as bbox_json:\n",
    "        bbox_frames = json.load(bbox_json)\n",
    "        for frameId, bbox_data in enumerate(bbox_frames['Frames']):\n",
    "            # get each image frame\n",
    "            with Image.open(src_img_dir+str(frameId)+'.jpg') as img:\n",
    "                inferred_frame = draw_response(img, bbox_data, animal_target='cow')\n",
    "                inferred_frame = cv2.cvtColor(inferred_frame, cv2.COLOR_BGR2RGB)\n",
    "                # check each 50 frame\n",
    "                if frameId % 50 == 0:\n",
    "                    print(\"Finish Processing {} frame\".format(frameId))\n",
    "                    plt.imshow(inferred_frame)\n",
    "                    plt.title(\"Frame {}\".format(int(frameId)))\n",
    "                    plt.savefig('debug_imgs/check_{}.jpg'.format(frameId), dpi=200)\n",
    "                    lap = time.time()\n",
    "                    print('lap time: ', lap - start)\n",
    "                videoWriter.write(inferred_frame)\n",
    "                img.close()\n",
    "\n",
    "    videoWriter.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    bbox_json.close()\n",
    "    \n",
    "    #end time\n",
    "    end = time.time()\n",
    "    print('total time lapse', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "strange-algebra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrameRate: 30.006940695611938\n",
      "Finish Processing 0 frame\n",
      "lap time:  12.659148216247559\n",
      "Finish Processing 50 frame\n",
      "lap time:  54.166203022003174\n",
      "Finish Processing 100 frame\n",
      "lap time:  93.99472737312317\n",
      "Finish Processing 150 frame\n",
      "lap time:  133.61087489128113\n",
      "Finish Processing 200 frame\n",
      "lap time:  164.30949306488037\n",
      "Finish Processing 250 frame\n",
      "lap time:  198.20463848114014\n",
      "Finish Processing 300 frame\n",
      "lap time:  253.77612900733948\n",
      "Finish Processing 350 frame\n",
      "lap time:  308.12100982666016\n",
      "Finish Processing 400 frame\n",
      "lap time:  363.8781921863556\n",
      "Finish Processing 450 frame\n",
      "lap time:  440.2033941745758\n",
      "Finish Processing 500 frame\n",
      "lap time:  509.17523074150085\n",
      "Finish Processing 550 frame\n",
      "lap time:  567.7913026809692\n",
      "Finish Processing 600 frame\n",
      "lap time:  617.6669824123383\n",
      "Finish Processing 650 frame\n",
      "lap time:  675.2704281806946\n",
      "Finish Processing 700 frame\n",
      "lap time:  740.7550001144409\n",
      "Finish Processing 750 frame\n",
      "lap time:  797.0035147666931\n",
      "Finish Processing 800 frame\n",
      "lap time:  848.0211560726166\n",
      "Finish Processing 850 frame\n",
      "lap time:  917.8079795837402\n",
      "Finish Processing 900 frame\n",
      "lap time:  980.4494423866272\n",
      "Finish Processing 950 frame\n",
      "lap time:  1035.7698247432709\n",
      "Finish Processing 1000 frame\n",
      "lap time:  1068.5969784259796\n",
      "Finish Processing 1050 frame\n",
      "lap time:  1093.1121439933777\n",
      "Finish Processing 1100 frame\n",
      "lap time:  1136.5213813781738\n",
      "Finish Processing 1150 frame\n",
      "lap time:  1207.2531998157501\n",
      "Finish Processing 1200 frame\n",
      "lap time:  1281.5180923938751\n",
      "Finish Processing 1250 frame\n",
      "lap time:  1374.0698862075806\n",
      "total time lapse 1430.3178277015686\n",
      "finished analyzing the video IMG_1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#, 'cattle_multi_1'\n",
    "video_name_list = ['IMG_1200']\n",
    "video_format = ['.mov']\n",
    "for v_idx, video in enumerate(video_name_list):\n",
    "    six_class_arn = ''\n",
    "    src_video = 'video_data/input_video/'+video+video_format[v_idx]\n",
    "    src_bbox_json = 'json_data/'+video+'_bbox.json'\n",
    "    src_img_dir = 'frame_img/'+video+'/'\n",
    "    output_video = 'video_data/inferred_video/inferred_'+video+'.mp4'\n",
    "\n",
    "    analyzeVideo(src_video, src_bbox_json, src_img_dir, output_video, six_class_arn)\n",
    "    print('finished analyzing the video '+video)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-entity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36] *",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
