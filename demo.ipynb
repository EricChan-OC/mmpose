{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_down_video_demo_full_frame_without_det.py\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from mmpose.apis import (vis_pose_result)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference.py\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from mmpose.core.post_processing import oks_nms\n",
    "from mmpose.datasets.pipelines import Compose\n",
    "from mmpose.models import build_posenet\n",
    "from mmpose.utils.hooks import OutputHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "horizontal-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_down.py\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import numpy as np\n",
    "from mmcv.image import imwrite\n",
    "from mmcv.visualization.image import imshow\n",
    "\n",
    "# from .. import builder\n",
    "# from ..registry import POSENETS\n",
    "# from .base import BasePose\n",
    "\n",
    "try:\n",
    "    from mmcv.runner import auto_fp16\n",
    "except ImportError:\n",
    "    warnings.warn('auto_fp16 from mmpose will be deprecated from v0.15.0'\n",
    "                  'Please install mmcv>=1.1.4')\n",
    "    from mmpose.core import auto_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-title",
   "metadata": {},
   "source": [
    "## Inference on ideos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerous-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pose_model(config, checkpoint=None, device='cuda:0'):\n",
    "    \"\"\"Initialize a pose model from config file.\n",
    "\n",
    "    Args:\n",
    "        config (str or :obj:`mmcv.Config`): Config file path or the config\n",
    "            object.\n",
    "        checkpoint (str, optional): Checkpoint path. If left as None, the model\n",
    "            will not load any weights.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed detector.\n",
    "    \"\"\"\n",
    "    if isinstance(config, str):\n",
    "        config = mmcv.Config.fromfile(config)\n",
    "    elif not isinstance(config, mmcv.Config):\n",
    "        raise TypeError('config must be a filename or Config object, '\n",
    "                        f'but got {type(config)}')\n",
    "    config.model.pretrained = None\n",
    "    model = build_posenet(config.model)\n",
    "    if checkpoint is not None:\n",
    "        # load model checkpoint\n",
    "        load_checkpoint(model, checkpoint, map_location=device)\n",
    "    # save the config in the model for convenience\n",
    "    model.cfg = config\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pharmaceutical-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xyxy2xywh(bbox_xyxy):\n",
    "    \"\"\"Transform the bbox format from x1y1x2y2 to xywh.\n",
    "\n",
    "    Args:\n",
    "        bbox_xyxy (np.ndarray): Bounding boxes (with scores), shaped (n, 4) or\n",
    "            (n, 5). (left, top, right, bottom, [score])\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Bounding boxes (with scores),\n",
    "          shaped (n, 4) or (n, 5). (left, top, width, height, [score])\n",
    "    \"\"\"\n",
    "    bbox_xywh = bbox_xyxy.copy()\n",
    "    bbox_xywh[:, 2] = bbox_xywh[:, 2] - bbox_xywh[:, 0] + 1\n",
    "    bbox_xywh[:, 3] = bbox_xywh[:, 3] - bbox_xywh[:, 1] + 1\n",
    "\n",
    "    return bbox_xywh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImage:\n",
    "    \"\"\"A simple pipeline to load image.\"\"\"\n",
    "\n",
    "    def __init__(self, color_type='color', channel_order='rgb'):\n",
    "        self.color_type = color_type\n",
    "        self.channel_order = channel_order\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"Call function to load images into results.\n",
    "\n",
    "        Args:\n",
    "            results (dict): A result dict contains the img_or_path.\n",
    "\n",
    "        Returns:\n",
    "            dict: ``results`` will be returned containing loaded image.\n",
    "        \"\"\"\n",
    "        if isinstance(results['img_or_path'], str):\n",
    "            results['image_file'] = results['img_or_path']\n",
    "            img = mmcv.imread(results['img_or_path'], self.color_type,\n",
    "                              self.channel_order)\n",
    "        elif isinstance(results['img_or_path'], np.ndarray):\n",
    "            results['image_file'] = ''\n",
    "            if self.color_type == 'color' and self.channel_order == 'rgb':\n",
    "                img = cv2.cvtColor(results['img_or_path'], cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            raise TypeError('\"img_or_path\" must be a numpy array or a str or '\n",
    "                            'a pathlib.Path object')\n",
    "\n",
    "        results['img'] = img\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _box2cs(cfg, box):\n",
    "    \"\"\"This encodes bbox(x,y,w,h) into (center, scale)\n",
    "\n",
    "    Args:\n",
    "        x, y, w, h\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing center and scale.\n",
    "\n",
    "        - np.ndarray[float32](2,): Center of the bbox (x, y).\n",
    "        - np.ndarray[float32](2,): Scale of the bbox w & h.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, w, h = box[:4]\n",
    "    input_size = cfg.data_cfg['image_size']\n",
    "    aspect_ratio = input_size[0] / input_size[1]\n",
    "    center = np.array([x + w * 0.5, y + h * 0.5], dtype=np.float32)\n",
    "\n",
    "    if w > aspect_ratio * h:\n",
    "        h = w * 1.0 / aspect_ratio\n",
    "    elif w < aspect_ratio * h:\n",
    "        w = h * aspect_ratio\n",
    "\n",
    "    # pixel std is 200.0\n",
    "    scale = np.array([w / 200.0, h / 200.0], dtype=np.float32)\n",
    "\n",
    "    scale = scale * 1.25\n",
    "\n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "italian-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inference_single_pose_model(model,\n",
    "                                 img_or_path,\n",
    "                                 bboxes,\n",
    "                                 dataset,\n",
    "                                 return_heatmap=False):\n",
    "\n",
    "    cfg = model.cfg\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # build the data pipeline\n",
    "    channel_order = cfg.test_pipeline[0].get('channel_order', 'rgb')\n",
    "    test_pipeline = [LoadImage(channel_order=channel_order)\n",
    "                     ] + cfg.test_pipeline[1:]\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "\n",
    "    assert len(bboxes[0]) in [4, 5]\n",
    "\n",
    "    flip_pairs = None\n",
    "   \n",
    "    if dataset in 'AnimalHorse10Dataset':\n",
    "        flip_pairs = []\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    batch_data = []\n",
    "    for bbox in bboxes:\n",
    "        center, scale = _box2cs(cfg, bbox)\n",
    "\n",
    "        # prepare data\n",
    "        data = {\n",
    "            'img_or_path':\n",
    "            img_or_path,\n",
    "            'center':\n",
    "            center,\n",
    "            'scale':\n",
    "            scale,\n",
    "            'bbox_score':\n",
    "            bbox[4] if len(bbox) == 5 else 1,\n",
    "            'bbox_id':\n",
    "            0,  # need to be assigned if batch_size > 1\n",
    "            'dataset':\n",
    "            dataset,\n",
    "            'joints_3d':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'joints_3d_visible':\n",
    "            np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),\n",
    "            'rotation':\n",
    "            0,\n",
    "            'ann_info': {\n",
    "                'image_size': np.array(cfg.data_cfg['image_size']),\n",
    "                'num_joints': cfg.data_cfg['num_joints'],\n",
    "                'flip_pairs': flip_pairs\n",
    "            }\n",
    "        }\n",
    "        data = test_pipeline(data)\n",
    "        batch_data.append(data)\n",
    "\n",
    "    batch_data = collate(batch_data, samples_per_gpu=1)\n",
    "\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter not work so just move image to cuda device\n",
    "        batch_data['img'] = batch_data['img'].to(device)\n",
    "    # get all img_metas of each bounding box\n",
    "    batch_data['img_metas'] = [\n",
    "        img_metas[0] for img_metas in batch_data['img_metas'].data\n",
    "    ]\n",
    "\n",
    "    # forward the model\n",
    "    with torch.no_grad():\n",
    "        result = model(\n",
    "            img=batch_data['img'],\n",
    "            img_metas=batch_data['img_metas'],\n",
    "            return_loss=False,\n",
    "            return_heatmap=return_heatmap)\n",
    "\n",
    "    return result['preds'], result['output_heatmap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "covered-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_top_down_pose_model(model,\n",
    "                                  img_or_path,\n",
    "                                  person_results,\n",
    "                                  bbox_thr=None,\n",
    "                                  format='xywh',\n",
    "                                  dataset='TopDownCocoDataset',\n",
    "                                  return_heatmap=False,\n",
    "                                  outputs=None):\n",
    "\n",
    "    # only two kinds of bbox format is supported.\n",
    "    assert format in ['xyxy', 'xywh']\n",
    "\n",
    "    pose_results = []\n",
    "    returned_outputs = []\n",
    "\n",
    "    if len(person_results) == 0:\n",
    "        return pose_results, returned_outputs\n",
    "\n",
    "    # Change for-loop preprocess each bbox to preprocess all bboxes at once.\n",
    "    bboxes = np.array([box['bbox'] for box in person_results])\n",
    "\n",
    "    # Select bboxes by score threshold\n",
    "    if bbox_thr is not None:\n",
    "        assert bboxes.shape[1] == 5\n",
    "        valid_idx = np.where(bboxes[:, 4] > bbox_thr)[0]\n",
    "        bboxes = bboxes[valid_idx]\n",
    "        person_results = [person_results[i] for i in valid_idx]\n",
    "\n",
    "    if format == 'xyxy':\n",
    "        bboxes_xyxy = bboxes\n",
    "        bboxes_xywh = _xyxy2xywh(bboxes)\n",
    "    else:\n",
    "        # format is already 'xywh'\n",
    "        bboxes_xywh = bboxes\n",
    "        bboxes_xyxy = _xywh2xyxy(bboxes)\n",
    "\n",
    "    # if bbox_thr remove all bounding box\n",
    "    if len(bboxes_xywh) == 0:\n",
    "        return [], []\n",
    "    print('bboxes', bboxes_xywh)\n",
    "    with OutputHook(model, outputs=outputs, as_tensor=False) as h:\n",
    "        # poses is results['pred'] # N x 17x 3\n",
    "        poses, heatmap = _inference_single_pose_model(\n",
    "            model,\n",
    "            img_or_path,\n",
    "            bboxes_xywh,\n",
    "            dataset,\n",
    "            return_heatmap=return_heatmap)\n",
    "\n",
    "        if return_heatmap:\n",
    "            h.layer_outputs['heatmap'] = heatmap\n",
    "\n",
    "        returned_outputs.append(h.layer_outputs)\n",
    "\n",
    "    assert len(poses) == len(person_results), print(\n",
    "        len(poses), len(person_results), len(bboxes_xyxy))\n",
    "    for pose, person_result, bbox_xyxy in zip(poses, person_results,\n",
    "                                              bboxes_xyxy):\n",
    "        pose_result = person_result.copy()\n",
    "        pose_result['keypoints'] = pose\n",
    "        pose_result['bbox'] = bbox_xyxy\n",
    "        pose_results.append(pose_result)\n",
    "\n",
    "    return pose_results, returned_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "streaming-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/demo_videos/cattle_single_2.mov\n",
      "vis_cattle_single_2.mov\n"
     ]
    }
   ],
   "source": [
    "cattle_part = 'leg_back'\n",
    "# set arguments\n",
    "args = {'device':'cuda:0', 'out_video_root': 'inference_result/video_result/cattle_'+cattle_part, \n",
    "       'pose_checkpoint': 'temp_logs/cattle_'+cattle_part+'/resnet/best.pth',\n",
    "       'pose_config': 'myConfigs/train_'+cattle_part+'_resnet.py',\n",
    "       'show':False, 'video_path': 'data/demo_videos/cattle_single_2.mov', \n",
    "       'kpt_thr': 0.6}\n",
    "print(args['video_path'])\n",
    "temp_vpath = args['video_path']\n",
    "print(f'vis_{os.path.basename(temp_vpath)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "violent-activation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "dataset AnimalHorse10Dataset\n",
      "cap <VideoCapture 0x7f1e21c33b10>\n",
      "fps 30.00598921940507\n",
      "frame size (1920, 1080)\n"
     ]
    }
   ],
   "source": [
    "pose_model = init_pose_model(\n",
    "        args['pose_config'], args['pose_checkpoint'], device=args['device'].lower())\n",
    "temp_vpath = args['video_path']\n",
    "dataset = pose_model.cfg.data['test']['type']\n",
    "print('dataset', dataset)\n",
    "cap = cv2.VideoCapture(temp_vpath)\n",
    "assert cap.isOpened(), f'Faild to load video file {temp_vpath}'\n",
    "print('cap', cap)\n",
    "fps = cap.get(5)\n",
    "print('fps', fps)\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "print('frame size', size)\n",
    "if args['out_video_root'] == '':\n",
    "    save_out_video = False\n",
    "else:\n",
    "    os.makedirs(args['out_video_root'], exist_ok=True)\n",
    "    save_out_video = True\n",
    "\n",
    "if save_out_video:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    videoWriter = cv2.VideoWriter(\n",
    "        os.path.join(args['out_video_root'],\n",
    "                     f'vis_{os.path.basename(temp_vpath)}'), fourcc,\n",
    "        fps, size)\n",
    "\n",
    "# optional\n",
    "return_heatmap = False\n",
    "\n",
    "# e.g. use ('backbone', ) to return backbone feature\n",
    "output_layer_names = None\n",
    "\n",
    "pose_res_json = {'result': []}\n",
    "pose_res_list = []\n",
    "img_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "circular-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bboxes [[   0    0 1921 1081]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConfigDict' object has no attribute 'test_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-35b172c9b80f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mreturn_heatmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_heatmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         outputs=output_layer_names)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# update data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtemp_img_pose\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0d6e8bed4a44>\u001b[0m in \u001b[0;36minference_top_down_pose_model\u001b[0;34m(model, img_or_path, person_results, bbox_thr, format, dataset, return_heatmap, outputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mbboxes_xywh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             return_heatmap=return_heatmap)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_heatmap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-99c756e3851d>\u001b[0m in \u001b[0;36m_inference_single_pose_model\u001b[0;34m(model, img_or_path, bboxes, dataset, return_heatmap)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# build the data pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mchannel_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_pipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channel_order'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rgb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     test_pipeline = [LoadImage(channel_order=channel_order)\n\u001b[1;32m     13\u001b[0m                      ] + cfg.test_pipeline[1:]\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConfigDict' object has no attribute 'test_pipeline'"
     ]
    }
   ],
   "source": [
    "while (cap.isOpened()):\n",
    "    flag, img = cap.read()\n",
    "    if not flag:\n",
    "        break\n",
    "    # current frame image pose result\n",
    "    temp_img_pose = {'id': img_idx, 'keypoints': []}\n",
    "    # keep the person class bounding boxes.\n",
    "    person_results = [{'bbox': np.array([0, 0, size[0], size[1]])}]\n",
    "\n",
    "    # test a single image, with a list of bboxes.\n",
    "    pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        img,\n",
    "        person_results,\n",
    "        format='xyxy',\n",
    "        dataset=dataset,\n",
    "        return_heatmap=return_heatmap,\n",
    "        outputs=output_layer_names)\n",
    "    # update data\n",
    "    temp_img_pose['id'] = img_idx\n",
    "    temp_img_pose['keypoints'] = pose_results[0]['keypoints'].tolist()\n",
    "    if img_idx % 100 == 0:\n",
    "        print('complete ', img_idx, ' frames.')\n",
    "    img_idx+=1\n",
    "\n",
    "    # show the results\n",
    "    vis_img = vis_pose_result(\n",
    "        pose_model,\n",
    "        img,\n",
    "        pose_results,\n",
    "        dataset=dataset,\n",
    "        kpt_score_thr=args['kpt_thr'],\n",
    "        show=False)\n",
    "\n",
    "    if args['show']:\n",
    "        cv2.imshow('Image', vis_img)\n",
    "\n",
    "    if save_out_video:\n",
    "        videoWriter.write(vis_img)\n",
    "    # save current pose result\n",
    "    pose_res_list.append(temp_img_pose)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "if save_out_video:\n",
    "    videoWriter.release()\n",
    "cv2.destroyAllWindows()\n",
    "# update pose result list\n",
    "print('frames count: ', len(pose_res_list))\n",
    "pose_res_json['result'] = pose_res_list\n",
    "# save json data\n",
    "with open('./video_'+cattle_part+'.json', 'w') as json_file:\n",
    "    json.dump(pose_res_json, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-apache",
   "metadata": {},
   "source": [
    "## Visualize Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-centre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
